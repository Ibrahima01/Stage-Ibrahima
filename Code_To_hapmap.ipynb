{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4443/4125819099.py:5: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filename)\n",
      "[Stage 14:=> (8 + 4) / 19][Stage 15:>   (0 + 0) / 4][Stage 16:>  (0 + 0) / 19]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=>(11 + 4) / 19][Stage 15:>   (0 + 0) / 4][Stage 16:>  (0 + 0) / 19]\r"
     ]
    }
   ],
   "source": [
    "# Chemin vers le fichier CSV\n",
    "filename = \"nouveau_fichier.csv\"\n",
    "\n",
    "# Charger le fichier CSV dans un dataframe pandas\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# Supprimer les 23 dernières lignes\n",
    "df = df.iloc[:-24]\n",
    "\n",
    "# Écrire les données modifiées dans le fichier CSV\n",
    "df.to_csv(\"newGPL18545_humanexome-12v1-1_a.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('GSE148375_processed_data.txt') as f:\n",
    "    # Lecture de la première ligne pour récupérer les noms des colonnes\n",
    "    colonnes = f.readline().strip().split('\\t')\n",
    "    \n",
    "    # Renommer le ID_REF par exm#\n",
    "    colonnes[0]=\"exm#\"\n",
    "    \n",
    "    # Création d'une nouvelle chaîne de caractères pour la première ligne modifiée\n",
    "    new_first_line = '\\t'.join(colonnes) + '\\n'\n",
    "    \n",
    "    # Écriture de la première ligne modifiée dans un nouveau fichier\n",
    "    with open('newGSE148375_processed_data.txt', 'w') as fw:\n",
    "        fw.write(new_first_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 15:39:03 WARN Utils: Your hostname, legrand-Aspire-A315-54K resolves to a loopback address: 127.0.1.1; using 192.168.58.128 instead (on interface wlp2s0)\n",
      "23/04/09 15:39:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 15:39:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Création d'une session Spark\n",
    "spark = SparkSession.builder.appName(\"fusion_data\").getOrCreate()\n",
    "\n",
    "# Chargement du fichier GPL18545_humanexome-12v1-1_a.csv dans un DataFrame\n",
    "df_gpl = spark.read.csv(\"newGPL18545_humanexome-12v1-1_a.csv\", sep=\",\",header=True, inferSchema=True)\n",
    "\n",
    "# Sélection des colonnes nécessaires\n",
    "df_gpl_select = df_gpl.select(\"IlmnID\", \"SNP\", \"Chr\", \"MapInfo\", \"RefStrand\")\n",
    "\n",
    "# Chargement du fichier GSE148375_processed_data.txt dans un DataFrame\n",
    "df_gse = spark.read.csv(\"GSE148375_processed_data.txt\", sep=\"\\t\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+---+------------+---------+\n",
      "|              IlmnID|  SNP|Chr|     MapInfo|RefStrand|\n",
      "+--------------------+-----+---+------------+---------+\n",
      "|exm-IND1-20044998...|[D/I]|  1|2.02183358E8|        -|\n",
      "|exm-IND1-20145348...|[D/I]|  1|2.03186865E8|        -|\n",
      "|exm-IND1-85310248...|[I/D]|  1| 8.5537661E7|        +|\n",
      "|exm-IND10-1028177...|[I/D]| 10|1.02827758E8|        +|\n",
      "|exm-IND10-1832963...|[D/I]| 10| 1.8289634E7|        -|\n",
      "+--------------------+-----+---+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_gpl_select.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renommer les noms des colonnes\n",
    "df_gpl_select = df_gpl_select.withColumnRenamed(\"IlmnID\", \"exm#\") \\\n",
    "                             .withColumnRenamed(\"SNP\", \"alleles\") \\\n",
    "                             .withColumnRenamed(\"Chr\", \"chrom\") \\\n",
    "                             .withColumnRenamed(\"MapInfo\", \"pos\") \\\n",
    "                             .withColumnRenamed(\"RefStrand\", \"strand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gse_select = df_gse.withColumnRenamed(\"ID_REF\", \"exm#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def remove_ends(input_string):\n",
    "    if len(input_string) < 3:\n",
    "        return \"input_string\"\n",
    "    else:\n",
    "        return input_string[1:-1]\n",
    "\n",
    "# Créer une fonction UDF pour appliquer la fonction remove_ends à une colonne\n",
    "udf_remove_ends = udf(remove_ends, StringType())\n",
    "\n",
    "def remove_ends_column(df, column_name):\n",
    "    # Ajouter une nouvelle colonne avec les valeurs modifiées\n",
    "    df = df.withColumn(column_name+\"_new\", udf_remove_ends(df[column_name]))\n",
    "    # Remplacer la colonne existante par la nouvelle colonne\n",
    "    df = df.drop(column_name).withColumnRenamed(column_name+\"_new\", column_name)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpl_select=remove_ends_column(df_gpl_select, \"alleles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Ajout de nouvelles colonnes avec des valeurs \"NA\"\n",
    "df_gpl_select = df_gpl_select.withColumn(\"assembly#\", lit(\"NA\")) \\\n",
    "                             .withColumn(\"center\", lit(\"NA\")) \\\n",
    "                             .withColumn(\"protLSID\", lit(\"NA\")) \\\n",
    "                             .withColumn(\"assayLSID\", lit(\"NA\")) \\\n",
    "                             .withColumn(\"panelLSID\", lit(\"NA\")) \\\n",
    "                             .withColumn(\"QCcode\", lit(\"NA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "242901"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gpl_select.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+------+-------+---------+------+--------+---------+---------+------+\n",
      "|                exm#|chrom|         pos|strand|alleles|assembly#|center|protLSID|assayLSID|panelLSID|QCcode|\n",
      "+--------------------+-----+------------+------+-------+---------+------+--------+---------+---------+------+\n",
      "|exm-IND1-20044998...|    1|2.02183358E8|     -|    D/I|       NA|    NA|      NA|       NA|       NA|    NA|\n",
      "|exm-IND1-20145348...|    1|2.03186865E8|     -|    D/I|       NA|    NA|      NA|       NA|       NA|    NA|\n",
      "|exm-IND1-85310248...|    1| 8.5537661E7|     +|    I/D|       NA|    NA|      NA|       NA|       NA|    NA|\n",
      "|exm-IND10-1028177...|   10|1.02827758E8|     +|    I/D|       NA|    NA|      NA|       NA|       NA|    NA|\n",
      "|exm-IND10-1832963...|   10| 1.8289634E7|     -|    D/I|       NA|    NA|      NA|       NA|       NA|    NA|\n",
      "+--------------------+-----+------------+------+-------+---------+------+--------+---------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_gpl_select.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jointure des deux DataFrames sur la colonne \"IlmnID\"\n",
    "df_join = df_gpl_select.join(df_gse_select, on=\"exm#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:============>                                             (4 + 4) / 19]\r"
     ]
    }
   ],
   "source": [
    "df_join.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:========================================================(19 + 0) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/09 02:44:39 WARN DAGScheduler: Broadcasting large task binary with size 1363.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fusionner toutes les partitions en une seule\n",
    "df_join = df_join.coalesce(1)\n",
    "\n",
    "# Enregistrer les données dans un seul fichier CSV\n",
    "df_join.write.format(\"csv\").option(\"header\", True).option(\"delimiter\", \"\\t\").save(\"hapmap.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
